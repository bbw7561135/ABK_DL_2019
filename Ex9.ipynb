{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: Decorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a simple dataset. The signal consists of 5 uncorrelated Gaussian distributions, the background is flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sig = np.random.normal(size=(2000,5))\n",
    "bg = np.random.uniform(low=-3, high=3, size=(2000,5))\n",
    "\n",
    "data = np.concatenate([sig,bg]) # combine signal and background\n",
    "labels = np.concatenate([np.ones(2000), np.zeros(2000)]) # and also create labels - one for signal and zero for background\n",
    "\n",
    "# shuffle the examples and indices\n",
    "idx = np.arange(data.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "data = data[idx]\n",
    "labels = labels[idx]\n",
    "\n",
    "# select subsets as training and testing data\n",
    "X_train = data[:1500]\n",
    "y_train = labels[:1500]\n",
    "\n",
    "X_test = data[1500:]\n",
    "y_test = labels[1500:]\n",
    "\n",
    "# and visualize the distributions\n",
    "def plot2d(xdim, ydim):\n",
    "    plt.clf()\n",
    "    plt.scatter(sig[:,xdim],sig[:,ydim],color='blue',alpha=0.5)\n",
    "    plt.scatter(bg[:,ydim],bg[:,xdim],color='orange',alpha=0.5)\n",
    "    plt.xlabel(\"Dim {0}\".format(xdim))\n",
    "    plt.ylabel(\"Dim {0}\".format(ydim))\n",
    "    plt.show()\n",
    "    \n",
    "plot2d(0,1)\n",
    "plot2d(0,2)\n",
    "plot2d(0,3)\n",
    "plot2d(0,4)\n",
    "plot2d(1,2)\n",
    "plot2d(1,3)\n",
    "plot2d(1,4)\n",
    "plot2d(2,3)\n",
    "plot2d(2,4)\n",
    "plot2d(3,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define neural network and training hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # DEFINE THE NETWORK LAYERS\n",
    "        # 1st LAYER should have the same size as inputs, last layer=1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "        # IMPLEMENT FORWARD PASS \n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Which device to use for NN calculations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "# Create network object\n",
    "model = NeuralNet().to(device)\n",
    "\n",
    "# Adam optimiser\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# how many epochs to train for\n",
    "n_epochs = 20 \n",
    "\n",
    "# how many examples / batch\n",
    "batch_size = 5\n",
    "\n",
    "# Keep track of the accuracies\n",
    "train_accs = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop and evaluation below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_examples = X_train.shape[0]\n",
    "n_batches = int(train_examples/batch_size)\n",
    "\n",
    "# Loop over the epochs\n",
    "for ep in range(n_epochs):\n",
    "    \n",
    "    # Each epoch is a complete loop over the training data\n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        i_start = i*batch_size\n",
    "        i_stop  = (i+1)*batch_size\n",
    "        \n",
    "        # Convert x and y to proper objects for PyTorch\n",
    "        x = torch.tensor(X_train[i_start:i_stop],dtype=torch.float)\n",
    "        y = torch.tensor(y_train[i_start:i_stop],dtype=torch.long)\n",
    "\n",
    "        \n",
    "        # Apply the network \n",
    "        net_out = model(x)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(net_out,y.float())\n",
    "        \n",
    "    \n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    # end of loop over batches\n",
    "        \n",
    "    # Calculate predictions on training and testing data\n",
    "    y_pred_train = model(torch.tensor(X_train,dtype=torch.float)).detach().numpy()[:,0]\n",
    "    y_pred_test  = model(torch.tensor(X_test,dtype=torch.float)).detach().numpy()[:,0]\n",
    "    \n",
    "    # Calculate accuracy on training and testing data\n",
    "    train_acc = sum(y_train.astype(bool) == (y_pred_train>0.5)) / y_train.shape[0]\n",
    "    test_acc = sum(y_test.astype(bool) == (y_pred_test>0.5)) / y_test.shape[0]\n",
    "    \n",
    "    # print some information\n",
    "    print(\"Epoch:\",ep, \"Train Accuracy:\", train_acc,  \"Test Accuracy:\", test_acc)\n",
    "    \n",
    "    # and store the accuracy for later use\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "# end of loop over epochs\n",
    "    \n",
    "# Prepare and show accuracy over time\n",
    "plt.axis('on')\n",
    "plt.plot(train_accs,label=\"train\")\n",
    "plt.plot(test_accs,label=\"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Best test accuracy:\",max(test_accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate the jensen shannon divergence\n",
    "def calc_js(P,Q,nbins=25):\n",
    "\n",
    "    # find lowest and highest value\n",
    "    low = min(min(P), min(Q))\n",
    "    high = max(max(P), max(Q))\n",
    "\n",
    "    # turn distributions into histograms\n",
    "    P_h,_ = np.histogram(P, bins=nbins, range=(low,high),density=True)\n",
    "    Q_h,_ = np.histogram(Q, bins=nbins, range=(low,high),density=True)\n",
    "\n",
    "    # calculate average histgrogram\n",
    "    M_h = 0.5 * (P_h + Q_h)\n",
    "    \n",
    "    # and add KL divergences\n",
    "    JS = 0.5 * stats.entropy(P_h,M_h) + 0.5 * stats.entropy(Q_h,M_h)\n",
    "    \n",
    "    return JS\n",
    "\n",
    "# Evaluate DNN on testing data\n",
    "x = torch.tensor(X_test,dtype=torch.float)\n",
    "y_pred = model(x).detach().numpy()[:,0]\n",
    "\n",
    "# Find the threshold corresponding to 70% efficiency for true signal events\n",
    "threshold = np.percentile(y_pred[y_test==1],70)\n",
    "\n",
    "# select events that pass (ie lie above that threshold)\n",
    "passed_selection = (y_pred > threshold)\n",
    "\n",
    "# Plot the distribution of all events before and after the selection\n",
    "_ = plt.hist(X_test[:,0],bins=20,density=True,range=(0,4),alpha=0.5)\n",
    "_ = plt.hist(X_test[passed_selection,0],bins=20,density=True,range=(0,4),alpha=0.5)\n",
    "\n",
    "# and calculate the jensen shannon divergence\n",
    "calc_js(X_test[:,0],X_test[passed_selection,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "\n",
    "  * Implement a fully connected network using all 5 input variables. What is the best accuracy it obtains and how strong does it affect the distribution of variable 0 (the code to draw the distribution and calculate the JS divergence are provided above)?\n",
    "  \n",
    "  * By how much does the accuracy and shaping change if you do NOT use the first input variable. You will need to change the network and all instances where the data is passed to the model. You can use `X[:,1:]` to select the sub-tensor without the first column.\n",
    "  \n",
    "  * Calculate weights that flatten the distribution of the first input variable (but use it as input to the network). Helpful numpy functions are `np.histogram` and `np.digitize`. The `binary_cross_entropy` loss functions has an argument `weight` that accepts a tensor with the same shape as `y` that contains the weight. Calculate the weight for each example and use in training. How much does this change accuracy and shaping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
